"""
Main entry point for the data processing pipeline with both single-run and scheduled modes.
"""

import os
import sys
import logging
import signal
from datetime import datetime
from dotenv import load_dotenv

# Add the src directory to the Python path
sys.path.append(os.path.join(os.path.dirname(__file__), ".."))

# Import both pipeline and scheduler
from src.ingestion.news_fetcher import fetch_news
from src.ingestion.stellar_fetcher import get_asset_volume, get_network_overview
from src.validators import validate_news_article, validate_onchain_metric
from src.analytics.market_analyzer import MarketAnalyzer, MarketData
from src.analytics.market_analyzer import get_explanation
from src.anomaly_detector import AnomalyDetector
from scheduler import AnalyticsScheduler

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    handlers=[
        logging.FileHandler("./logs/data_processor.log"),
        logging.StreamHandler(sys.stdout),
    ],
)

logger = logging.getLogger(__name__)

# Module-level detector so it accumulates rolling window data across
# scheduled pipeline runs (meaningful baselines build up over time).
anomaly_detector = AnomalyDetector(window_size_hours=24, z_threshold=2.5)

# Global scheduler instance
scheduler = None


def setup_signal_handlers():
    """Setup signal handlers for graceful shutdown"""

    def signal_handler(sig, frame):
        logger.info("Received shutdown signal, cleaning up...")
        if scheduler:
            scheduler.stop()
        sys.exit(0)

    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)


def run_data_pipeline():
    """Run a single execution of the complete data processing pipeline."""
    print("=" * 60)
    print("DATA PROCESSING PIPELINE")
    print("=" * 60)
    print(f"Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print()

    try:
        # Step 1: Fetch news data
        print("1. FETCHING CRYPTO NEWS")
        print("-" * 40)

        raw_news_articles = fetch_news(limit=5)
        print(f"Fetched {len(raw_news_articles)} news articles (raw)")

        # Validate and sanitize news articles
        news_articles = []
        for idx, article in enumerate(raw_news_articles):
            validated = validate_news_article(article)
            if validated:
                news_articles.append(validated.dict())
            else:
                logger.warning(f"Dropped invalid news article at index {idx}")

        print(f"Validated {len(news_articles)} news articles")

        # Calculate average sentiment (mock - in real scenario, use sentiment engine)
        if news_articles:
            # Mock sentiment calculation (replace with actual sentiment analysis)
            mock_sentiment = 0.3  # Placeholder
            print(f"Mock sentiment score: {mock_sentiment:.2f}")
        else:
            mock_sentiment = 0.0
            print("No valid news articles, using neutral sentiment")

        # Step 2: Fetch Stellar on-chain data
        print("\n2. FETCHING STELLAR ON-CHAIN DATA")
        print("-" * 40)


        # Get XLM volume for last 24 hours
        raw_volume_24h = get_asset_volume("XLM", hours=24)
        validated_volume_24h = validate_onchain_metric({
            "metric_id": "xlm_volume_24h",
            "value": raw_volume_24h.get("total_volume", 0.0),
            "timestamp": raw_volume_24h.get("end_time", ""),
            "chain": "stellar",
            "extra": raw_volume_24h,
        })
        if validated_volume_24h:
            volume_24h = validated_volume_24h.dict()
        else:
            logger.warning("Invalid on-chain metric for 24h volume, using defaults.")
            volume_24h = {"total_volume": 0.0, "transaction_count": 0}

        print(f"XLM Volume (24h): {volume_24h.get('total_volume', 0.0):,.2f}")
        print(f"Transactions: {volume_24h.get('transaction_count', 0)}")

        # Get XLM volume for last 48 hours for comparison
        raw_volume_48h = get_asset_volume("XLM", hours=48)
        validated_volume_48h = validate_onchain_metric({
            "metric_id": "xlm_volume_48h",
            "value": raw_volume_48h.get("total_volume", 0.0),
            "timestamp": raw_volume_48h.get("end_time", ""),
            "chain": "stellar",
            "extra": raw_volume_48h,
        })
        if validated_volume_48h:
            volume_48h = validated_volume_48h.dict()
        else:
            logger.warning("Invalid on-chain metric for 48h volume, using defaults.")
            volume_48h = {"total_volume": 0.0}

        # Calculate volume change percentage
        if volume_48h["total_volume"] > 0:
            volume_change = (
                volume_24h["total_volume"] - volume_48h["total_volume"]
            ) / volume_48h["total_volume"]
            print(f"Volume Change (24h vs 48h): {volume_change:.2%}")
        else:
            volume_change = 0.0
            print("Insufficient data for volume change calculation")

        # Get network overview
        network_stats = get_network_overview()
        if network_stats:
            print(f"Latest Ledger: {network_stats.get('latest_ledger', 'N/A')}")
            print(f"Transaction Count: {network_stats.get('transaction_count', 0)}")

        # Step 3: Market Analysis
        print("\n3. MARKET ANALYSIS")
        print("-" * 40)

        # Create market data
        market_data = MarketData(
            sentiment_score=mock_sentiment, volume_change=volume_change
        )

        # Analyze market trend
        trend, score, metrics = MarketAnalyzer.analyze_trend(market_data)

        print(f"Market Health Score: {score:.2f}")
        print(f"Trend: {trend.value.upper()}")
        print(f"Sentiment Component: {metrics['sentiment_component']:.2f}")
        print(f"Volume Component: {metrics['volume_component']:.2f}")

        # Generate explanation
        explanation = get_explanation(score, trend)
        print(f"\nAnalysis: {explanation}")

        # Step 4: Anomaly Detection
        print("\n4. ANOMALY DETECTION")
        print("-" * 40)

        current_volume = float(volume_24h["total_volume"])
        now = datetime.utcnow()

        # Feed current data point into the rolling window detector
        anomaly_detector.add_data_point(
            volume=current_volume,
            sentiment_score=mock_sentiment,
            timestamp=now,
        )

        # Run detection on both metrics
        volume_anomaly = anomaly_detector.detect_volume_anomaly(current_volume, now)
        sentiment_anomaly = anomaly_detector.detect_sentiment_anomaly(mock_sentiment, now)

        anomalies_found = []

        for result in [volume_anomaly, sentiment_anomaly]:
            status = "⚠️  ANOMALY" if result.is_anomaly else "✓  Normal"
            print(
                f"{status} | {result.metric_name.capitalize():<10} | "
                f"value={result.current_value:.4f} | "
                f"z={result.z_score:.2f} | "
                f"severity={result.severity_score:.2f}"
            )
            if result.is_anomaly:
                anomalies_found.append(result.to_dict())
                logger.warning(
                    f"Anomaly detected — metric={result.metric_name}, "
                    f"value={result.current_value:.4f}, "
                    f"z_score={result.z_score:.2f}, "
                    f"severity={result.severity_score:.2f}"
                )

        window_stats = anomaly_detector.get_window_stats()
        print(f"Detector window: {window_stats['data_points_count']} data points")

        if not anomalies_found:
            print("No anomalies detected in current pipeline run.")

        # Step 5: Output summary
        print("\n5. PIPELINE SUMMARY")
        print("-" * 40)
        print(f"✓ News Articles Processed: {len(news_articles)}")
        print(f"✓ XLM Volume Analyzed: {volume_24h['total_volume']:,.2f}")
        print(f"✓ Market Trend: {trend.value.upper()}")
        print(f"✓ Anomalies Detected: {len(anomalies_found)}")
        print(f"✓ Analysis Complete: {datetime.now().strftime('%H:%M:%S')}")

        result = {
            "success": True,
            "news_count": len(news_articles),
            "volume_xlm": volume_24h["total_volume"],
            "market_trend": trend.value,
            "health_score": score,
            "anomalies": anomalies_found,
            "timestamp": datetime.now().isoformat(),
        }

        logger.info(f"Pipeline completed successfully: {result}")
        return result

    except Exception as e:
        error_msg = f"Pipeline Error: {e}"
        print(f"\n❌ {error_msg}")
        import traceback

        traceback.print_exc()
        logger.error(error_msg, exc_info=True)
        return {
            "success": False,
            "error": str(e),
            "timestamp": datetime.now().isoformat(),
        }


def start_scheduler():
    """Start the scheduled data processing service."""
    global scheduler

    logger.info("=" * 70)
    logger.info("LumenPulse Data Processing Service Starting")
    logger.info("=" * 70)

    try:
        # Initialize and start the scheduler
        scheduler = AnalyticsScheduler(run_data_pipeline)
        setup_signal_handlers()

        # Option to run immediately on startup (useful for testing)
        run_on_startup = os.getenv("RUN_IMMEDIATELY", "false").lower() == "true"

        if run_on_startup:
            logger.info("Running analyzer immediately on startup...")
            scheduler.run_immediately()

        # Start the scheduler
        scheduler.start()

        logger.info("Data processing service is running. Press Ctrl+C to stop.")
        logger.info("The Market Analyzer will run automatically every hour.")

        # Keep the application running
        import time

        while True:
            time.sleep(1)

    except Exception as e:
        logger.error(f"Fatal error in data processing service: {e}", exc_info=True)
        if scheduler:
            scheduler.stop()
        sys.exit(1)


def main():
    """Main entry point - handles both CLI modes"""
    load_dotenv()

    # Create logs directory if it doesn't exist
    os.makedirs("./logs", exist_ok=True)

    # Check command line arguments
    if len(sys.argv) > 1:
        command = sys.argv[1].lower()

        if command == "run":
            # Run pipeline once and exit
            return run_data_pipeline()
        elif command == "serve":
            # Start scheduled service
            start_scheduler()
        elif command == "help":
            print("Usage:")
            print("  python pipeline.py run     - Run pipeline once")
            print("  python pipeline.py serve   - Start scheduled service")
            print("  python pipeline.py help    - Show this help")
            return {"help": True}
        else:
            print(f"Unknown command: {command}")
            print("Use 'python pipeline.py help' for usage instructions")
            return {"error": f"Unknown command: {command}"}
    else:
        # Default: run once (original behavior)
        result = run_data_pipeline()
        print("\n" + "=" * 60)
        print("PIPELINE COMPLETE")
        print("=" * 60)
        return result


if __name__ == "__main__":
    result = main()
    if result and result.get("help"):
        sys.exit(0)
    elif result and not result.get("success", True):
        sys.exit(1)